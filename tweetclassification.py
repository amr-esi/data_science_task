# -*- coding: utf-8 -*-
"""tweetClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q-rJ2zmEw6UhvKEQ_jfHtadJ9YN5qrrf
"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas as pd 
import re
import nltk
from textblob import TextBlob
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize, word_tokenize
nltk.download('punkt')
nltk.download('wordnet')
from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer

df=pd.read_csv("train.csv")
df.head(4)

def removeTagUrlLow(sentence):
  #remove tag
  new_sentence=re.sub(r'@\w+', '', sentence)
  #remove url
  new_sentence=re.sub(r'http.?://[^\s]+[\s]?', '',new_sentence)
  #to lowerCase
  #return(new_sentence.lower())
  return(new_sentence)
  
#removeTagUrlLow('@rraina1481 I fEar so')

import nltk
nltk.download('averaged_perceptron_tagger')

def keepNV(sentence):
  tagged_sentence = nltk.tag.pos_tag(sentence.split())
  edited_sentence = [word for word,tag in tagged_sentence if tag == "NN" or tag =="VB"]
  return(' '.join(edited_sentence))

keepNV('hi dear Naima I may love you hard best better sister')

from spacy.lang.en import English
from spacy.lang.en.stop_words import STOP_WORDS
new_stopwords=["video","penalty","great","form","picture","month","link","city","blog","hour","years","debut","number","moment","http","night","adress","country","series","news","tomorrow","indvaus","time","today","year","tonight","week","a","i","You","you","he","she","it","we","they","re","u","about","above","after","again","against","ain","all","am","an","and","any","are","aren","aren't","as","at","be","because","been","before","being","below","between","both","but","by","can","couldn","couldn't","d","did","didn","didn't","do","does","doesn","doesn't","doing","don","don't","down","during","each","few","for","from","further","had","hadn","hadn't","has","hasn","hasn't","have","haven","haven't","having","he","her","here","hers","herself","him","himself","his","how","i","if","in","into","is","isn","isn't","it","it's","its","itself","just","ll","m","ma","me","mightn","mightn't","more","most","mustn","mustn't","my","myself","needn","needn't","no","nor","not","now","o","of","off","on","once","only","or","other","our","ours","ourselves","out","over","own","re","s","same","shan","shan't","she","she's","should","should've","shouldn","shouldn't","so","some","such","t","than","that","that'll","the","their","theirs","them","themselves","then","there","these","they","this","those","through","to","too","under","until","up","ve","very","was","wasn","wasn't","we","were","weren","weren't","what","when","where","which","while","who","whom","why","will","with","won","won't","wouldn","wouldn't","y","you","you'd","you'll","you're","you've","your","yours","yourself","yourselves","could","he'd","he'll","he's","here's","how's","i'd","i'll","i'm","i've","let's","ought","she'd","she'll","that's","there's","they'd","they'll","they're","they've","we'd","we'll","we're","we've","what's","when's","where's","who's","why's","would","able","abst","accordance","according","accordingly","across","act","actually","added","adj","affected","affecting","affects","afterwards","ah","almost","alone","along","already","also","although","always","among","amongst","announce","another","anybody","anyhow","anymore","anyone","anything","anyway","anyways","anywhere","apparently","approximately","arent","arise","around","aside","ask","asking","auth","available","away","awfully","b","back","became","become","becomes","becoming","beforehand","begin","beginning","beginnings","begins","behind","believe","beside","besides","beyond","biol","brief","briefly","c","ca","came","cannot","can't","cause","causes","certain","certainly","co","com","come","comes","contain","containing","contains","couldnt","date","different","done","downwards","due","e","ed","edu","effect","eg","eight","eighty","either","else","elsewhere","end","ending","enough","especially","et","etc","even","ever","every","everybody","everyone","everything","everywhere","ex","except","f","far","ff","fifth","first","five","fix","followed","following","follows","former","formerly","forth","found","four","furthermore","g","gave","get","gets","getting","give","given","gives","giving","go","goes","gone","got","gotten","h","happens","hardly","hed","hence","hereafter","hereby","herein","heres","hereupon","hes","hi","hid","hither","home","howbeit","however","hundred","id","ie","im","immediate","immediately","importance","important","inc","indeed","index","information","instead","invention","inward","itd","it'll","j","k","keep","keeps","kept","kg","km","know","known","knows","l","largely","last","lately","later","latter","latterly","least","less","lest","let","lets","like","liked","likely","line","little","'ll","look","looking","looks","ltd","made","mainly","make","makes","many","may","maybe","mean","means","meantime","meanwhile","merely","mg","might","million","miss","ml","moreover","mostly","mr","mrs","much","mug","must","n","na","name","namely","nay","nd","near","nearly","necessarily","necessary","need","needs","neither","never","nevertheless","new","next","nine","ninety","nobody","non","none","nonetheless","noone","normally","nos","noted","nothing","nowhere","obtain","obtained","obviously","often","oh","ok","okay","old","omitted","one","ones","onto","ord","others","otherwise","outside","overall","owing","p","page","pages","part","particular","particularly","past","per","perhaps","placed","please","plus","poorly","possible","possibly","potentially","pp","predominantly","present","previously","primarily","probably","promptly","proud","provides","put","q","que","quickly","quite","qv","r","ran","rather","rd","readily","really","recent","recently","ref","refs","regarding","regardless","regards","related","relatively","research","respectively","resulted","resulting","results","right","run","said","saw","say","saying","says","sec","section","see","seeing","seem","seemed","seeming","seems","seen","self","selves","sent","seven","several","shall","shed","shes","show","showed","shown","showns","shows","significant","significantly","similar","similarly","since","six","slightly","somebody","somehow","someone","somethan","something","sometime","sometimes","somewhat","somewhere","soon","sorry","specifically","specified","specify","specifying","still","stop","strongly","sub","substantially","successfully","sufficiently","suggest","sup","sure","take","taken","taking","tell","tends","th","thank","thanks","thanx","thats","that've","thence","thereafter","thereby","thered","therefore","therein","there'll","thereof","therere","theres","thereto","thereupon","there've","theyd","theyre","think","thou","though","thoughh","thousand","throug","throughout","thru","thus","til","tip","together","took","toward","towards","tried","tries","truly","try","trying","ts","twice","two","u","un","unfortunately","unless","unlike","unlikely","unto","upon","ups","us","use","used","useful","usefully","usefulness","uses","using","usually","v","value","various","'ve","via","viz","vol","vols","vs","w","want","wants","wasnt","way","wed","welcome","went","werent","whatever","what'll","whats","whence","whenever","whereafter","whereas","whereby","wherein","wheres","whereupon","wherever","whether","whim","whither","whod","whoever","whole","who'll","whomever","whos","whose","widely","willing","wish","within","without","wont","words","world","wouldnt","www","x","yes","yet","youd","youre","z","zero","a's","ain't","allow","allows","apart","appear","appreciate","appropriate","associated","best","better","c'mon","c's","cant","changes","clearly","concerning","consequently","consider","considering","corresponding","course","currently","definitely","described","despite","entirely","exactly","example","going","greetings","hello","help","hopefully","ignored","inasmuch","indicate","indicated","indicates","inner","insofar","it'd","keep","keeps","novel","presumably","reasonably","second","secondly","sensible","final","serious","seriously","sure","t's","third","thorough","thoroughly","three","well","wonder","rt","pm"]

def removeStopSPACY(sentence):
  # Load English tokenizer, tagger, parser, NER and word vectors
  nlp = English()
  #  "nlp" Object is used to create documents with linguistic annotations.
  my_doc = nlp(sentence)
  # Create list of word tokens
  token_list = []
  for token in my_doc:
    token_list.append(token.text)
  # Create list of word tokens after removing stopwords
  filtered_sentence =[] 
  for word in token_list:
    lexeme = nlp.vocab[word]
    if lexeme.is_stop == False:
        filtered_sentence.append(word) 
  return(" ".join(filtered_sentence))



def removeStopNLTK(list_tokenizer):
  #remove stowords
  stop_words = set(stopwords.words('english'))
  new_sentence=[]
  for w in list_tokenizer:
    if w not in stop_words:
      if w not in new_stopwords:
         new_sentence.append(w)
  results=" ".join(new_sentence)
  return(results) # return a string


removeStopNLTK("hello u my He are is great final night friend hi lol".split())
#removeStopSPACY("bonjour u my He friend hi after lol")

def removePunStopNum(sentence):
  
  #remove numbers 
  result=re.sub(r'\d+','',sentence)
  ##remove all  spaces : big ,between end of words 
  #bet
  result=re.sub(r"\s+", " ",result, flags=re.UNICODE)
  #big & end 
  result=re.sub("^\s+|\s+$", "",result, flags=re.UNICODE)
  #########print(result)
  #remove punctiations
  tokenizer = RegexpTokenizer(r'\w+')
  output= tokenizer.tokenize(result)
  #########print(output)
  result=removeStopNLTK(output)
  return(result)

removePunStopNum("hi: my friend 19934   gta")

def StemLemTweet(sentence):
  porter = PorterStemmer()
  lancaster=LancasterStemmer()
  wordnet_lemmatizer = WordNetLemmatizer()
  token_words=word_tokenize(sentence)
  stem_sentence=[]
  for word in token_words:
    stem_sentence.append(wordnet_lemmatizer.lemmatize(word))
    stem_sentence.append(" ")
    
  return "".join(stem_sentence)

x=StemLemTweet("I'm very talking to mam about cats ")
print(x)
# porter:['american', 'deepest', 'dollar', 'fear', 'measur', 'seckerri', 'term', 'valu']
#lancaster:['deepest', 'doll', 'fear', 'meas', 'seckerry', 'term', 'valu'] 
#df.head(3)

def preprocessing(df):
  #remove tags url and convert to lowercase 
  df['processText'] = df['TweetText'].apply(lambda x: removeTagUrlLow(x))
  df['processText'] = df['processText'].apply(lambda x: keepNV(x))
  df['processText'] = df['processText'].apply(lambda x: x.lower())
#spelling correction 
#df['processText'] = df['processText'].apply(lambda x: str(TextBlob(x).correct()))
#remove punctuations stopwords and numbers
  df['processText'] = df['processText'].apply(lambda x: removePunStopNum(x))

##KEEP NOUN & VERB
#remove less than 4 caracter
  df['processText'] = df['processText'].apply(lambda x: re.sub(r'\b\w{1,3}\b', '',x))   
#stemming and lemmazation 
  df['processText'] = df['processText'].apply(lambda x: StemLemTweet(x))
##remove rare words
  freq = pd.Series(' '.join(df['processText']).split()).value_counts()[2000:]  #8429
  index = list(freq.index)
  df['processText'] = df['processText'].apply(lambda x: " ".join(x for x in x.split() if x not in index))

  df['processText'] = df['processText'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))

#### remove comon words

freq = pd.Series(' '.join(df['processText']).split()).value_counts()[850:]
freq
df['processText'] = df['processText'].apply(lambda x: " ".join(j for j in x.split() if j not in freq))
#re.sub(r'\b\w{1,3}\b', '', 'The run quick brown fox jumps over war the lazy dog')

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df[['TweetId','processText']],df['Label'], test_size=0.2)

# Count Vectors as features
vectorizer = CountVectorizer(stop_words='english')
#X = vectorizer.fit_transform(X_train['processText'])
vect_train=vectorizer.fit_transform(X_train['processText'])
vect_test=vectorizer.transform(X_test['processText'])
len(vectorizer.get_feature_names())
#TF-IDF Vectors as features

# word level tf-idf
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', max_features=839)

xtrain_tfidf =  tfidf_vect.transform(X_train['processText'])
xvalid_tfidf =  tfidf_vect.transform(X_test['processText'])

#SVM on TF IDF Vectors
from sklearn.svm import SVC ,LinearSVC
from sklearn.metrics import accuracy_score

#accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram,train_y, xvalid_tfidf_ngram)

#def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):
# fit the training dataset on the classifier
model=SVC(C=1.0, kernel='linear', gamma='auto').fit(xtrain_tfidf,y_train)
# predict the labels on validation dataset
predictions = model.predict(xvalid_tfidf)
accuracy=accuracy_score(predictions, y_test)
print ("SVM,Vectors: ", accuracy)   
    
   #kernel='linear',gamma='scale', decision_function_shape='ovo' 
#SVM,Vectors:  0.5164750957854406
#SVM,Vectors:  0.7770114942528735 linear

from sklearn.metrics import classification_report, confusion_matrix
df_test=pd.read_csv("test.csv")
preprocessing(df_test)
t=tfidf_vect.transform(df_test['processText'])
y_pred = model.predict(t)
#print(confusion_matrix(y_test, y_pred))
#print(classification_report(y_test, y_pred))
type(y_pred)

dfb = pd.DataFrame(y_pred)
df_test['Label']=dfb
df_test.head(20)
df_test[["TweetId","Label"]].to_csv("test_final.csv",index = False,sep=',' , line_terminator='\n')
dfd=pd.read_csv("test_final.csv")
dfd.columns

#basic feature extraction 
stop_words = set(stopwords.words('english'))
df['word_count'] = df['TweetText'].apply(lambda x: len(str(x).split(" ")))
df['char_count'] = df['TweetText'].str.len() ## it includes spaces
def avg_word(sentence):
  words = sentence.split()
  return (sum(len(word) for word in words)/len(words))

df['avg_word'] = df['TweetText'].apply(lambda x: avg_word(x))
df['stopwords'] = df['TweetText'].apply(lambda x: len([x for x in x.split() if x in stop_words]))
df['hastags'] = df['TweetText'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))
df['numerics'] = df['TweetText'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))
df['upper'] = df['TweetText'].apply(lambda x: len([x for x in x.split() if x.isupper()]))

df.head(4)